{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/explain-model/explain-on-amlcompute/regression-sklearn-on-amlcompute.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain Blackbox Models with SHAP on AMLCompute\n",
    "\n",
    "## Overview of Tutorial\n",
    "This notebook is Part 3 of a four part workshop that demonstrates how to use [InterpretML](interpret.ml) and [Fairlearn](fairlearn.org) (and their integrations with Azure Machine Learning) to understand and analyze models better. The different components of the workshop are as follows:\n",
    "\n",
    "- Part 1: [Interpretability with glassbox models (EBM)](https://github.com/microsoft/ResponsibleAI-Airlift/blob/main/Interpret/EBM/Interpretable%20Classification%20Methods.ipynb)\n",
    "- Part 2a: [Explain blackbox models with SHAP (and upload explanations to Azure Machine Learning)](https://github.com/microsoft/ResponsibleAI-Airlift/blob/main/Interpret/SHAP/explain-model-SHAP.ipynb)\n",
    "- Part 2b: [Run Interpretability on Azure Machine Learning](https://github.com/microsoft/ResponsibleAI-Airlift/blob/main/Interpret/SHAP/explain-model-Azure.ipynb) optional  (HERE)\n",
    "- Part 3: [Model fairness assessment and unfairness mitigation](https://github.com/microsoft/ResponsibleAI-Airlift/blob/main/Fairness/AI-fairness-Census.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook showcases how to train and explain a regression model remotely via Azure Machine Learning Compute (AMLCompute), and download the calculated explanations locally on your personal machine.\n",
    "It demonstrates the API calls that you need to make to submit a run for training and explaining a model to AMLCompute, and download the compute explanations remotely.\n",
    "\n",
    "We will showcase one of the tabular data explainers: TabularExplainer (SHAP).\n",
    "\n",
    "Problem: Boston Housing Price Prediction (Regression)\n",
    "\n",
    "\n",
    "\n",
    "![](./images/interpretability-architecture.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "[Interpret-Community](https://github.com/interpretml/interpret-community) is an experimental repository extending [Interpret](https://github.com/interpretml/interpret), with additional interpretability techniques, utility functions, and a visualization dashboard to handle real-world datasets and workflows for explaining models trained on tabular data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall interpret-community -y\n",
    "%pip install --upgrade interpret-community[visualization]\n",
    "%pip install --upgrade azureml-interpret\n",
    "%pip install --upgrade azureml-contrib-interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing packages, you must close and reopen the notebook as well as restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect To Workspace\n",
    "\n",
    "Just like in the previous tutorials, we will need to connect to a [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class)?view=azure-ml-py).\n",
    "\n",
    "The following code will allow you to create a workspace if you don't already have one created. You must have an Azure subscription to create a workspace:\n",
    "\n",
    "```python\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.create(name='myworkspace',\n",
    "                      subscription_id='<azure-subscription-id>',\n",
    "                      resource_group='myresourcegroup',\n",
    "                      create_resource_group=True,\n",
    "                      location='eastus2')\n",
    "```\n",
    "\n",
    "**If you are running this on a Notebook VM, you can import the existing workspace.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** that the above commands reads a config.json file that exists by default within the Notebook VM. If you are running this locally or want to use a different workspace, you must add a config file to your project directory. The config file should have the following schema:\n",
    "\n",
    "```\n",
    "    {\n",
    "        \"subscription_id\": \"<SUBSCRIPTION-ID>\",\n",
    "        \"resource_group\": \"<RESOURCE-GROUP>\",\n",
    "        \"workspace_name\": \"<WORKSPACE-NAME>\"\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create An Experiment\n",
    "\n",
    "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'explainer-remote-airlift20'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to AmlCompute\n",
    "\n",
    "Azure Machine Learning Compute is managed compute infrastructure that allows the user to easily create single to multi-node compute of the appropriate VM Family. It is created **within your workspace region** and is a resource that can be used by other users in your workspace. It autoscales by default to the max_nodes, when a job is submitted, and executes in a containerized environment packaging the dependencies as specified by the user. \n",
    "\n",
    "Since it is managed compute, job scheduling and cluster management are handled internally by Azure Machine Learning service. \n",
    "\n",
    "For more information on Azure Machine Learning Compute, please read [this article](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)\n",
    "\n",
    "If you are an existing BatchAI customer who is migrating to Azure Machine Learning, please read [this article](https://aka.ms/batchai-retirement)\n",
    "\n",
    "**Note**: As with other Azure services, there are limits on certain resources (for eg. AmlCompute quota) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota.\n",
    "\n",
    "\n",
    "The training script train_explain.py is already created for you. Let's have a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Compute Target\n",
    "\n",
    "A [compute target](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.computetarget?view=azure-ml-py) is a designated compute resource/environment where you run your training script or host your service deployment. This location may be your local machine or a cloud-based compute resource. Compute targets can be reused across the workspace for different runs and experiments. \n",
    "\n",
    "**If you completed tutorial 1 of this series, then you should have already created a compute target and can skip this step**\n",
    "\n",
    "Otherwise, run the cell below to create an auto-scaling [Azure Machine Learning Compute](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.amlcompute?view=azure-ml-py) cluster, which is a managed-compute infrastructure that allows the user to easily create a single or multi-node compute. To create the cluster, we need to specify the following parameters:\n",
    "\n",
    "- `vm_size`: The is the type of GPUs that we want to use in our cluster. For this tutorial, we will use **Standard_NC12s_v3 (NVIDIA V100) GPU Machines** .\n",
    "- `idle_seconds_before_scaledown`: This is the number of seconds before a node will scale down in our auto-scaling cluster. We will set this to **6000** seconds. \n",
    "- `min_nodes`: This is the minimum numbers of nodes that the cluster will have. To avoid paying for compute while they are not being used, we will set this to **0** nodes.\n",
    "- `max_modes`: This is the maximum number of nodes that the cluster will scale up to. Will will set this to **2** nodes.\n",
    "\n",
    "**When jobs are submitted to the cluster it takes approximately 5 minutes to allocate new nodes** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "AmlCompute.supported_vmsizes(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Experiment Run\n",
    "\n",
    "Now that our compute is ready, we can begin to submit our run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create project directory\n",
    "\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "project_folder = './Airlift-explainer-remote-run-on-amlcompute'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "shutil.copy('train_explain.py', project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision a compute target\n",
    "You can provision an AmlCompute resource by simply defining two parameters thanks to smart defaults. By default it autoscales from 0 nodes and provisions dedicated VMs to run your job in a container. This is useful when you want to continously re-use the same target, debug it between jobs or simply share the resource with other users of your workspace.\n",
    "\n",
    "- vm_size: VM family of the nodes provisioned by AmlCompute. Simply choose from the supported_vmsizes() above\n",
    "- max_nodes: Maximum nodes to autoscale to while running a job on AmlCompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"custcompute\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a new RunConfig object\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute target created in previous step\n",
    "run_config.target = cpu_cluster.name\n",
    "\n",
    "# Enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "azureml_pip_packages = [\n",
    "    'azureml-defaults', 'azureml-contrib-interpret', 'azureml-core', 'azureml-telemetry',\n",
    "    'azureml-interpret', 'azureml-dataprep'\n",
    "]\n",
    "\n",
    "# Note: this is to pin the scikit-learn and pandas versions to be same as notebook.\n",
    "# In production scenario user would choose their dependencies\n",
    "import pkg_resources\n",
    "available_packages = pkg_resources.working_set\n",
    "sklearn_ver = None\n",
    "pandas_ver = None\n",
    "for dist in available_packages:\n",
    "    if dist.key == 'scikit-learn':\n",
    "        sklearn_ver = dist.version\n",
    "    elif dist.key == 'pandas':\n",
    "        pandas_ver = dist.version\n",
    "sklearn_dep = 'scikit-learn'\n",
    "pandas_dep = 'pandas'\n",
    "if sklearn_ver:\n",
    "    sklearn_dep = 'scikit-learn=={}'.format(sklearn_ver)\n",
    "if pandas_ver:\n",
    "    pandas_dep = 'pandas=={}'.format(pandas_ver)\n",
    "# Specify CondaDependencies obj\n",
    "# The CondaDependencies specifies the conda and pip packages that are installed in the environment\n",
    "# the submitted job is run in.  Note the remote environment(s) needs to be similar to the local\n",
    "# environment, otherwise if a model is trained or deployed in a different environment this can\n",
    "# cause errors.  Please take extra care when specifying your dependencies in a production environment.\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=[sklearn_dep, pandas_dep],\n",
    "                                                                            pip_packages=azureml_pip_packages)\n",
    "\n",
    "from azureml.core import Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder, \n",
    "                      script='train_explain.py', \n",
    "                      run_config=run_config) \n",
    "run = experiment.submit(config=src)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Explanations\n",
    "Download model explanations from Azure Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient\n",
    "\n",
    "# Get model explanation data\n",
    "client = ExplanationClient.from_run(run)\n",
    "global_explanation = client.download_model_explanation()\n",
    "local_importance_values = global_explanation.local_importance_values\n",
    "expected_values = global_explanation.expected_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or you can use the saved run.id to retrive the feature importance values\n",
    "client = ExplanationClient.from_run_id(ws, experiment_name, run.id)\n",
    "global_explanation = client.download_model_explanation()\n",
    "local_importance_values = global_explanation.local_importance_values\n",
    "expected_values = global_explanation.expected_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top k (e.g., 4) most important features with their importance values\n",
    "global_explanation_topk = client.download_model_explanation(top_k=4)\n",
    "global_importance_values = global_explanation_topk.get_ranked_global_values()\n",
    "global_importance_names = global_explanation_topk.get_ranked_global_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('global importance values: {}'.format(global_importance_values))\n",
    "print('global importance names: {}'.format(global_importance_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top k (e.g., 4) most important features with their importance values\n",
    "global_explanation = client.download_model_explanation()\n",
    "global_importance_values = global_explanation.get_ranked_global_values()\n",
    "global_importance_names = global_explanation.get_ranked_global_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('global importance values: {}'.format(global_importance_values))\n",
    "print('global importance names: {}'.format(global_importance_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(global_explanation.get_feature_importance_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve model for visualization and deployment\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "original_model = Model(ws, 'model_explain_model_on_amlcomp')\n",
    "model_path = original_model.download(exist_ok=True)\n",
    "original_model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve x_test for visualization\n",
    "import joblib\n",
    "x_test_path = './x_test_boston_housing.pkl'\n",
    "run.download_file('x_test_boston_housing.pkl', output_file_path=x_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = joblib.load('x_test_boston_housing.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "Load the visualization dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret_community.widget import ExplanationDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(global_explanation, original_model, datasetX=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "authors": [
   {
    "name": "mesameki"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('fastai': conda)",
   "language": "python",
   "name": "python361064bitfastaiconda126e9dc5e9d047d39b9a45d9bce3ec8a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
